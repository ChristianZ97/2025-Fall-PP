{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeDxDbRKJZEh",
        "outputId": "234b8b2b-b3ab-412d-8e6f-ce78892ce75d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install flash-attn Package (About 20 Min)\n",
        "!pip install flash-attn==1.0.9 --no-build-isolation -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "!nvidia-smi\n",
        "print(\"CUDA Usage :\", torch.cuda.is_available())\n",
        "print(\"GPU :\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHHIo8QTKBDj",
        "outputId": "4d63949d-dbd9-4f23-e7b1-fce3e7654675"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov 26 15:49:52 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "CUDA Usage : True\n",
            "GPU : Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import json\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from einops import rearrange\n",
        "from flash_attn.flash_attn_interface import flash_attn_unpadded_qkvpacked_func\n",
        "\n",
        "# ---------------------------\n",
        "# FLOPs and Efficiency Calculation\n",
        "# ---------------------------\n",
        "def flops(batch_size, seq_len, head_dim, num_heads, causal, mode='fwd'):\n",
        "    assert mode in ['fwd', 'bwd', 'fwd_bwd']\n",
        "    f = 4 * batch_size * seq_len**2 * num_heads * head_dim // (2 if causal else 1)\n",
        "    return f if mode == 'fwd' else (2.5 * f if mode == 'bwd' else 3.5 * f)\n",
        "\n",
        "def efficiency(flop, time):\n",
        "    return (flop / time / 10**12) if time > 0 else 0.0\n",
        "\n",
        "# ---------------------------\n",
        "# Custom Benchmark Function\n",
        "# ---------------------------\n",
        "def benchmark_fwd_bwd(func, qkv, cu_seqlens, dropout_p, causal, repeats=30):\n",
        "    fwd_times, bwd_times = [], []\n",
        "    for _ in range(repeats):\n",
        "        torch.cuda.synchronize()\n",
        "        start = torch.cuda.Event(enable_timing=True)\n",
        "        end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "        start.record()\n",
        "        out = func(qkv, cu_seqlens, qkv.shape[0], dropout_p, causal=causal)\n",
        "        end.record()\n",
        "        torch.cuda.synchronize()\n",
        "        fwd_times.append(start.elapsed_time(end) / 1000.0)\n",
        "\n",
        "        grad = torch.randn_like(out)\n",
        "        start.record()\n",
        "        out.backward(grad, retain_graph=True)\n",
        "        end.record()\n",
        "        torch.cuda.synchronize()\n",
        "        bwd_times.append(start.elapsed_time(end) / 1000.0)\n",
        "\n",
        "    return fwd_times, bwd_times\n",
        "\n",
        "# ---------------------------\n",
        "# PyTorch baseline attention\n",
        "# ---------------------------\n",
        "def pytorch_attn_func(qkv, dropout_p=0.0, causal=True):\n",
        "    batch_size, seq_len, _, num_heads, head_dim = qkv.shape\n",
        "    q, k, v = qkv.unbind(dim=2)\n",
        "    q = rearrange(q, 'b t h d -> (b h) t d')\n",
        "    k = rearrange(k, 'b s h d -> (b h) d s')\n",
        "    softmax_scale = 1.0 / math.sqrt(head_dim)\n",
        "\n",
        "    scores = torch.empty(batch_size * num_heads, seq_len, seq_len, dtype=qkv.dtype, device=qkv.device)\n",
        "    scores = rearrange(torch.baddbmm(scores, q, k, beta=0, alpha=softmax_scale),\n",
        "                       '(b h) t s -> b h t s', h=num_heads)\n",
        "    if causal:\n",
        "        causal_mask = torch.triu(torch.full((seq_len, seq_len), -10000.0, device=scores.device), 1)\n",
        "        scores = scores + causal_mask.to(dtype=scores.dtype)\n",
        "    attention = torch.softmax(scores, dim=-1)\n",
        "    attention_drop = F.dropout(attention, dropout_p)\n",
        "    output = torch.einsum('bhts,bshd->bthd', attention_drop , v)\n",
        "    return output.to(dtype=qkv.dtype)\n",
        "\n",
        "# ---------------------------\n",
        "# Main Benchmark Function\n",
        "# ---------------------------\n",
        "def benchmark_attention(batch_size, seq_len, num_heads, emb_dim, impl, causal, repeats, output):\n",
        "    assert impl in ['Pytorch', 'Flash1']\n",
        "    device = 'cuda'\n",
        "    dtype = torch.float16\n",
        "    dropout_p = 0.0\n",
        "    head_dim = emb_dim // num_heads\n",
        "\n",
        "    qkv = torch.randn(\n",
        "        batch_size, seq_len, 3, num_heads, head_dim,\n",
        "        device=device, dtype=dtype, requires_grad=True\n",
        "    )\n",
        "\n",
        "    if impl == 'Flash1':\n",
        "        total_len = batch_size * seq_len\n",
        "        cu_seqlens = torch.arange(0, (batch_size + 1) * seq_len, step=seq_len, dtype=torch.int32, device=device)\n",
        "        qkv_unpad = rearrange(qkv, 'b s three h d -> (b s) three h d')\n",
        "        attention_func = lambda x, cu, tot, dp, causal: flash_attn_unpadded_qkvpacked_func(\n",
        "            x, cu, tot, dropout_p=dp, causal=causal\n",
        "        )\n",
        "        input_tensor = qkv_unpad\n",
        "        cu_input = cu_seqlens\n",
        "    else:\n",
        "        attention_func = lambda x, cu, tot, dp, causal: pytorch_attn_func(x, dp, causal)\n",
        "        input_tensor = qkv\n",
        "        cu_input = None\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    fwd_times, bwd_times = benchmark_fwd_bwd(attention_func, input_tensor, cu_input, dropout_p, causal=causal, repeats=repeats)\n",
        "\n",
        "    forward_time = sum(fwd_times) / len(fwd_times)\n",
        "    backward_time = sum(bwd_times) / len(bwd_times)\n",
        "    peak_memory_usage = torch.cuda.max_memory_allocated() / (1024**2)\n",
        "\n",
        "    benchmark_result = {\n",
        "        'forward': {\n",
        "            'time(s)': forward_time,\n",
        "            'FLOPS(TFLOPs/s)': efficiency(\n",
        "                flops(batch_size, seq_len, head_dim, num_heads, causal, mode='fwd'),\n",
        "                forward_time\n",
        "            )\n",
        "        },\n",
        "        'backward': {\n",
        "            'time(s)': backward_time,\n",
        "            'FLOPS(TFLOPs/s)': efficiency(\n",
        "                flops(batch_size, seq_len, head_dim, num_heads, causal, mode='bwd'),\n",
        "                backward_time\n",
        "            )\n",
        "        },\n",
        "        'forward_backward': {\n",
        "            'time(s)': forward_time + backward_time,\n",
        "            'FLOPS(TFLOPs/s)': efficiency(\n",
        "                flops(batch_size, seq_len, head_dim, num_heads, causal, mode='fwd_bwd'),\n",
        "                forward_time + backward_time\n",
        "            )\n",
        "        },\n",
        "        'peak_memory_usage(MB)': peak_memory_usage,\n",
        "    }\n",
        "\n",
        "    with open(output, 'w') as json_file:\n",
        "        json.dump(benchmark_result, json_file, indent=2)\n",
        "\n",
        "    print(f\"Benchmark completed. Results saved to {output}\")\n",
        "    print(json.dumps(benchmark_result, indent=2))\n"
      ],
      "metadata": {
        "id": "RLnKC5KQTT9V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run benchmark for FlashAttention v1\n",
        "benchmark_attention(\n",
        "    batch_size=16,\n",
        "    seq_len=512,\n",
        "    num_heads=8,\n",
        "    emb_dim=512,\n",
        "    impl='Flash1',  # Choose between 'Pytorch' or 'Flash1'\n",
        "    causal=True,\n",
        "    repeats=30,\n",
        "    output='flash1_benchmark.json'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGKUjp_tKWOv",
        "outputId": "ea3f3107-989f-4f8f-ed46-f8af42090e7c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmark completed. Results saved to flash1_benchmark.json\n",
            "{\n",
            "  \"forward\": {\n",
            "    \"time(s)\": 0.0019403253237406412,\n",
            "    \"FLOPS(TFLOPs/s)\": 2.2135294754180603\n",
            "  },\n",
            "  \"backward\": {\n",
            "    \"time(s)\": 0.007021610633532207,\n",
            "    \"FLOPS(TFLOPs/s)\": 1.5291959068084304\n",
            "  },\n",
            "  \"forward_backward\": {\n",
            "    \"time(s)\": 0.008961935957272849,\n",
            "    \"FLOPS(TFLOPs/s)\": 1.6773591786047992\n",
            "  },\n",
            "  \"peak_memory_usage(MB)\": 112.0009765625\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}